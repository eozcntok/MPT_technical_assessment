{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d804e0-b92b-43b4-874a-cea8401dc8b7",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84a278-5b46-42f8-b50f-206406c054ef",
   "metadata": {},
   "source": [
    "Import required libraries (for the requirements please navigate to requirements.txt in the folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5938fd-becf-4728-9ee8-da7af84c4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import re\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import fastparquet\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3613a559-cc0b-49a9-8aae-1469d1ec5f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data path, chart path and number of files to be downloaded\n",
    "data_path = \"data\"\n",
    "chart_path = \"charts\"\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(chart_path, exist_ok=True)\n",
    "# The link to the data\n",
    "url = \"https://www.ons.gov.uk/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/timeseries/ap2y/lms/previous/\"\n",
    "base_url = \"https://www.ons.gov.uk\"\n",
    "number_of_files = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540fe7a-a186-420f-a878-c3a522ec9445",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80700671-d25b-456f-b01f-026dec646892",
   "metadata": {},
   "source": [
    "This section parses HTML to identify and collect all relevant csv data files. For each vintage, it generates a corresponding \n",
    "vacancy_data_{releasedate}.csv file containing the extracted data. The csv files are saved under \"/data\" folder. While csv is a widely used format and sufficient for these relatively small datasets, it becomes less efficient as data volume grows. For larger datasets, Parquet is a better choice due to its better read/write performance, file size compression and compatibility with big data tools like Spark. I added the relevant codes to also save the data as Parquet files named as vacancy_data_{releasedate}.parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42741ae8-e784-4c65-be8f-d1c5252c312b",
   "metadata": {},
   "source": [
    "### 1.1 Read Target Page Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142f36f3-9364-4ec6-a755-2d3b910ec425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send request with headers to mimic a browser\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "response = requests.get(url, headers=headers, verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32853c5-fd9d-4272-98d7-09b21c0507cf",
   "metadata": {},
   "source": [
    "### 1.2 Identify CSV Files in the Page Content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f68dc0-4485-4a98-992a-b4c670ae6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HTML\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> tags with the csv download class\n",
    "links = soup.find_all(\"a\", attrs={\"data-gtm-type\": \"download-version-csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ac2036d-93da-49ae-ac67-f7138f599974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv_file(file_link, base_url, data_path):\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from a constructed URL and saves it locally with a filename \n",
    "    based on the release date extracted from the file content.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "        file_link (bs4.element.Tag): A BeautifulSoup tag object representing an <a> element, \n",
    "                                     expected to contain 'href' and 'data-gtm-date' attributes.\n",
    "        base_url (str): The base URL to prepend to the 'href' to form the full download URL.\n",
    "        data_path (str): The local directory path where the CSV file will be saved.\n",
    "\n",
    "    Behavior:\n",
    "        - Constructs the full URL using `base_url` and the 'href' from `file_link`.\n",
    "        - Sends a GET request to download the CSV file.\n",
    "        - Extracts the release date from the file content using a regular expression.\n",
    "        - Formats the release date as 'YYYYMMDD' and uses it in the filename to make the files chronogically ordered.\n",
    "        - Saves the file to `data_path` with the name 'vacancy_data_<release_date>.csv' and 'vacancy_data_<release_date>.parquet'.\n",
    "        - If the release date is not found, defaults to 'vacancy_data_000.csv' and 'vacancy_data_000.parquet'.\n",
    "        - Prints status messages indicating success or failure.\n",
    "\n",
    "    Note:\n",
    "        SSL verification is disabled in the request (`verify=False`), which may pose a security risk.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    href = file_link.get(\"href\")\n",
    "    date_superseded = file_link.get(\"data-gtm-date\")\n",
    "    full_url = base_url + href\n",
    "    \n",
    "    print(f\"{date_superseded}: {full_url}\")\n",
    "\n",
    "    response = requests.get(full_url, verify=False)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Decode content to string\n",
    "        content = response.content.decode(\"utf-8\")\n",
    "\n",
    "        # Extract release date using regex\n",
    "        match = re.search(r'\"Release date\",\"([\\d\\-]+)\"', content)\n",
    "        if match:\n",
    "            release_date = match[1].replace(\"-\", \"\")  # e.g., \"12082025\"\n",
    "            parsed_date = datetime.strptime(release_date, \"%d%m%Y\")\n",
    "            release_date = parsed_date.strftime(\"%Y%m%d\") # Format the release date to YYYYMMDD so that downloaded files are named in chronological order\n",
    "            filename_csv = os.path.join(data_path, f\"vacancy_data_{release_date}.csv\")\n",
    "            filename_parquet = os.path.join(data_path, f\"vacancy_data_{release_date}.parquet\")\n",
    "        else:\n",
    "            filename_csv = os.path.join(data_path, f\"vacancy_data_000.csv\")\n",
    "            filename_parquet = os.path.join(data_path, f\"vacancy_data_000.parquet\")\n",
    "\n",
    "        # Save file\n",
    "        with open(filename_csv, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        # Save as Parquet\n",
    "        df = pd.read_csv(io.StringIO(content))\n",
    "        df.to_parquet(filename_parquet, engine=\"fastparquet\", index=False)\n",
    "\n",
    "        print(f\"Downloaded and saved as '{filename_csv}'\")\n",
    "    else:\n",
    "        print(f\"Failed to download {release_date}. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389b0ac3-a34f-4369-8d5a-5f89aeab89d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest: https://www.ons.gov.uk/generator?format=csv&uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/timeseries/ap2y/lms\n",
      "Downloaded and saved as 'data\\vacancy_data_20250916.csv'\n"
     ]
    }
   ],
   "source": [
    "# Download the first 24 files \n",
    "for link in links[:number_of_files]:\n",
    "    download_csv_file(link, base_url, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b47a2-1b40-4c2c-9e69-f6d3b7952c16",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning & Structuring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca9254e-2d8a-4a8a-9965-46da225e280d",
   "metadata": {},
   "source": [
    "This section brings together multiple files containing job vacancy data and combines them into one organized dataset. It starts by identifying the point (MAY 2001) in each file where consistent monthly data begins, then aligns all the data by month. Each file represents a different vintage. Once everything is aligned and sorted by date, the consolidated data is saved into the file \"vacancy_data_allvintages.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00de993c-23aa-4e15-a384-07e970750c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vacancy_csv(file):\n",
    "    \"\"\"\n",
    "    Processes a vacancy csv file by cleaning and transforming its contents into a time series format.\n",
    "\n",
    "    Args:\n",
    "        file (str): The csv file that will be processed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with:\n",
    "            - 'Date' as the index in 'YYYY-MM' format.\n",
    "            - One column named after the vintage date extracted from the filename.\n",
    "\n",
    "    Processing steps:\n",
    "        - Read the csv file.\n",
    "        - Cleans the 'Title' column by collapsing multiple spaces into one.\n",
    "        - Converts the 'Title' column to title case and parses it as a date in 'YYYY MMM' format.\n",
    "        - Drops rows where the 'Title' column could not be parsed as a date.\n",
    "        - Formats the parsed dates to 'YYYY-MM'.\n",
    "        - Infers a vintage identifier from the filename and renames the column accordingly.\n",
    "        - Sets the 'Date' column as the index.\n",
    "    \"\"\"\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv(os.path.join(data_path,file)) ##\n",
    "    # Read the parquet file\n",
    "    #df = pd.read_parquet(os.path.join(data_path,file))\n",
    "    \n",
    "    # Replace multiple spaces with one space in Title column\n",
    "    df['Title'] = df['Title'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "    # Convert Title column into date having YYYY MMM format\n",
    "    df[\"Title\"] = df[\"Title\"].str.title()\n",
    "    df[\"Title\"] = pd.to_datetime(df[\"Title\"], format='%Y %b', errors='coerce')\n",
    "    # Drop non-date columns\n",
    "    df = df.dropna(subset=\"Title\")\n",
    "    # Convert Date column into YY-MM format\n",
    "    df[\"Title\"] = df[\"Title\"].dt.strftime(\"%Y-%m\")\n",
    "    \n",
    "    # Infer the vintage date from the file name\n",
    "    vintage_date = file.replace(\"vacancy_data_\",\"v_\").replace(\".csv\", \"\") ##\n",
    "    #vintage_date = file.replace(\"vacancy_data_\",\"v_\").replace(\".parquet\", \"\") ##\n",
    "    # Rename columns as Date and vintage date identifier\n",
    "    df.columns = [\"Date\", vintage_date]\n",
    "    # Set Date as index\n",
    "    df = df.set_index(\"Date\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7881c3f-9fd3-4fe7-bcf7-0eb367ae3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all csv/parquet files in the current directory that start with \"vacancy_data\"\n",
    "data_files = [f for f in os.listdir(data_path) if f.startswith(\"vacancy_data\") and f.endswith(\".csv\")]\n",
    "#data_files = [f for f in os.listdir(data_path) if f.startswith(\"vacancy_data\") and f.endswith(\".parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4c47fcd-4dfe-4fbe-a68c-eb23bdcb8409",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = []\n",
    "for file in data_files:\n",
    "    df = process_vacancy_csv(file)\n",
    "    combined_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a0f28df-abfc-49dc-87c2-506dd04eef48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>v_20230912</th>\n",
       "      <th>v_20231114</th>\n",
       "      <th>v_20231212</th>\n",
       "      <th>v_20240116</th>\n",
       "      <th>v_20240213</th>\n",
       "      <th>v_20240312</th>\n",
       "      <th>v_20240416</th>\n",
       "      <th>v_20240514</th>\n",
       "      <th>v_20240611</th>\n",
       "      <th>...</th>\n",
       "      <th>v_20241217</th>\n",
       "      <th>v_20250121</th>\n",
       "      <th>v_20250218</th>\n",
       "      <th>v_20250320</th>\n",
       "      <th>v_20250415</th>\n",
       "      <th>v_20250513</th>\n",
       "      <th>v_20250610</th>\n",
       "      <th>v_20250717</th>\n",
       "      <th>v_20250812</th>\n",
       "      <th>v_20250916</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-05</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>...</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-06</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>...</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-07</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>...</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-08</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>...</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-09</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>...</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date v_20230912 v_20231114 v_20231212 v_20240116 v_20240213 v_20240312  \\\n",
       "0  2001-05        680        680        680        680        680        680   \n",
       "1  2001-06        674        674        674        674        674        674   \n",
       "2  2001-07        663        663        663        663        663        663   \n",
       "3  2001-08        663        663        663        663        663        663   \n",
       "4  2001-09        639        639        639        639        639        639   \n",
       "\n",
       "  v_20240416 v_20240514 v_20240611  ... v_20241217 v_20250121 v_20250218  \\\n",
       "0        680        680        680  ...        680        680        680   \n",
       "1        674        674        674  ...        674        674        674   \n",
       "2        663        663        663  ...        663        663        663   \n",
       "3        663        663        663  ...        663        663        663   \n",
       "4        639        639        639  ...        639        639        639   \n",
       "\n",
       "  v_20250320 v_20250415 v_20250513 v_20250610 v_20250717 v_20250812 v_20250916  \n",
       "0        680        680        680        680        680        680        680  \n",
       "1        674        674        674        674        674        674        674  \n",
       "2        663        663        663        663        663        663        663  \n",
       "3        663        663        663        663        663        663        663  \n",
       "4        639        639        639        639        639        639        639  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge dataframes in combined df list into a dataframe\n",
    "combined_df = pd.concat(combined_df, axis=1)\n",
    "combined_df = combined_df.sort_index().reset_index()\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f051285-f093-4d3a-8d8b-98026d1cc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the consolidated data as a csv/parquet file\n",
    "combined_df.to_csv(os.path.join(data_path, \"whole_vacancy_data.csv\"), index=False)\n",
    "#combined_df.to_parquet(os.path.join(data_path, \"whole_vacancy_data.parquet\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0b71f-fb42-4f30-b235-7de322a3f07b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
